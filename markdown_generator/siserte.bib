
@inproceedings{iserte_study_2022,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {A {Study} on the {Resource} {Utilization} and {User} {Behavior} on {Titan} {Supercomputer}},
	copyright = {All rights reserved},
	isbn = {978-3-030-96498-6},
	doi = {10.1007/978-3-030-96498-6_23},
	abstract = {Understanding HPC facilities users’ behaviors and how computational resources are requested and utilized is not only crucial for the cluster productivity but also essential for designing and constructing future exascale HPC systems.},
	language = {en},
	booktitle = {Driving {Scientific} and {Engineering} {Discoveries} {Through} the {Integration} of {Experiment}, {Big} {Data}, and {Modeling} and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Iserte, Sergio},
	editor = {Nichols, Jeffrey and Maccabe, Arthur ‘Barney’ and Nutaro, James and Pophale, Swaroop and Devineni, Pravallika and Ahearn, Theresa and Verastegui, Becky},
	year = {2022},
	keywords = {Data science, GPU, HPC, Scheduling, Workload},
	pages = {398--410},
}

@book{iserte_construya_2021,
	title = {Construya su propio supercomputador con {Raspberry} {Pi}},
	copyright = {All rights reserved},
	isbn = {978-84-267-3281-1},
	abstract = {¿Conoce la supercomputación? ¿Quiere introducirse en este campo de una forma entretenida? Sin darse cuenta, la supercomputación lo rodea en su día a día: los anuncios ajustados a sus gustos, las predicciones meteorológicas cada vez más exactas, la simulación del efecto del viento sobre un vehículo... Las empresas más influyentes, importantes avances científicos o, incluso, películas han mostrado la existencia de grandes centros de computación. No obstante, ¿sabe cómo funcionan estos sistemas, quién los usa, para qué y cómo se configuran? Construya su propio supercomputador con Raspberry Pi da respuesta a todas estas preguntas mientras lo guía en la configuración de un prototipo de supercomputador. Con este libro experimentará de primera mano la computación de altas prestaciones, desde el montaje físico del sistema hasta la ejecución de aplicaciones científicas. Así pues, gracias a esta lectura: - Aprenderá los conceptos básicos sobre la supercomputación. - Construirá su propio prototipo funcional de supercomputador utilizando hardware de bajo coste. - Configurará el sistema siguiendo patrones de diseño utilizados hoy en día en este tipo de infraestructuras. - Conocerá algunas de las herramientas clásicas en este contexto. - Instalará y ejecutará aplicaciones propias de distintas disciplinas científicas que se apoyan en la computación de altas prestaciones. - Descubrirá las tecnologías de virtualización y la utilidad de los contenedores en el mundo de la supercomputación. Este libro está escrito por cuatro entusiastas de la computación de altas prestaciones, por lo que podrá emprender su viaje hacia la supercomputación de la mano de Sergio Iserte Agut (investigador en la Universitat Jaume I y profesor de la Universitat Oberta de Catalunya), Sandra Catalán Pallarés (profesora e investigadora de la Universidad Complutense de Madrid), Rocío Carratalá Sáez (investigadora en la Universitat Jaume I) y Sergio López Huguet (investigador en la Universitat Politècnica de València). ¡Este es el momento! No dude en adquirir el libro para disfrutar aprendiendo sobre la supercomputación: una disciplina cuya importancia crece a pasos agigantados.},
	language = {Español},
	author = {Iserte, Sergio and Pallarés, Sandra Catalán and Sáez, Rocío Carratalá and Huguet, Sergio López},
	month = may,
	year = {2021},
}

@article{dolz_simulator_2012,
	title = {A simulator to assess energy saving strategies and policies in {HPC} workloads},
	volume = {46},
	copyright = {All rights reserved},
	issn = {0163-5980},
	url = {https://doi.org/10.1145/2331576.2331578},
	doi = {10.1145/2331576.2331578},
	abstract = {In recent years power consumption of high performance computing (HPC) clusters has become a growing problem due, e.g., to the economic cost of electricity, the emission of carbon dioxide (with negative impact on the environment), and the generation of heat (which reduces hardware reliability). In past work, we developed EnergySaving cluster, a software package that regulates the number of active nodes in an HPC facility to match the users' demands. In this paper, we extend this work by presenting a simulator for this tool that allows the evaluation and analysis of the benefits of applying different energy-saving strategies and policies, under realistic workloads, to different cluster configurations.},
	number = {2},
	urldate = {2024-01-23},
	journal = {ACM SIGOPS Operating Systems Review},
	author = {Dolz, Manuel F. and Fernández, Juan C. and Iserte, Sergio and Mayo, Rafael and Quintana-Ortí, Enrique S.},
	month = jul,
	year = {2012},
	pages = {2--9},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\I6KSRFUA\\Dolz et al. - 2012 - A simulator to assess energy saving strategies and.pdf:application/pdf},
}

@article{silla_benefits_2017,
	title = {On the benefits of the remote {GPU} virtualization mechanism: {The} {rCUDA} case},
	volume = {29},
	copyright = {Copyright © 2017 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	shorttitle = {On the benefits of the remote {GPU} virtualization mechanism},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4072},
	doi = {10.1002/cpe.4072},
	abstract = {Graphics processing units (GPUs) are being adopted in many computing facilities given their extraordinary computing power, which makes it possible to accelerate many general purpose applications from different domains. However, GPUs also present several side effects, such as increased acquisition costs as well as larger space requirements. They also require more powerful energy supplies. Furthermore, GPUs still consume some amount of energy while idle, and their utilization is usually low for most workloads. In a similar way to virtual machines, the use of virtual GPUs may address the aforementioned concerns. In this regard, the remote GPU virtualization mechanism allows an application being executed in a node of the cluster to transparently use the GPUs installed at other nodes. Moreover, this technique allows to share the GPUs present in the computing facility among the applications being executed in the cluster. In this way, several applications being executed in different (or the same) cluster nodes can share 1 or more GPUs located in other nodes of the cluster. Sharing GPUs should increase overall GPU utilization, thus reducing the negative impact of the side effects mentioned before. Reducing the total amount of GPUs installed in the cluster may also be possible. In this paper, we explore some of the benefits that remote GPU virtualization brings to clusters. For instance, this mechanism allows an application to use all the GPUs present in the computing facility. Another benefit of this technique is that cluster throughput, measured as jobs completed per time unit, is noticeably increased when this technique is used. In this regard, cluster throughput can be doubled for some workloads. Furthermore, in addition to increase overall GPU utilization, total energy consumption can be reduced up to 40\%. This may be key in the context of exascale computing facilities, which present an important energy constraint. Other benefits are related to the cloud computing domain, where a GPU can be easily shared among several virtual machines. Finally, GPU migration (and therefore server consolidation) is one more benefit of this novel technique.},
	language = {en},
	number = {13},
	urldate = {2024-01-23},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Silla, Federico and Iserte, Sergio and Reaño, Carlos and Prades, Javier},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.4072},
	keywords = {CUDA, GPU migration, GPU virtualization, InfiniBand, Slurm, Xen},
	pages = {e4072},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\5X9HT9NG\\Silla et al. - 2017 - On the benefits of the remote GPU virtualization m.pdf:application/pdf;Snapshot:C\:\\Users\\siser\\Zotero\\storage\\RKCRQNGI\\cpe.html:text/html},
}

@article{iserte_dmr_2018,
	title = {{DMR} {API}: {Improving} cluster productivity by turning applications into malleable},
	volume = {78},
	copyright = {All rights reserved},
	issn = {0167-8191},
	shorttitle = {{DMR} {API}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167819118302229},
	doi = {10.1016/j.parco.2018.07.006},
	abstract = {Adaptive workloads can change on–the–fly the configuration of their jobs, in terms of number of processes. To carry out these job reconfigurations, we have designed a methodology which enables a job to communicate with the resource manager and, through the runtime, to change its number of MPI ranks. The collaboration between both the workload manager—aware of the queue of jobs and the resources allocation—and the parallel runtime—able to transparently handle the processes and the program data—is crucial for our throughput-aware malleability methodology. Hence, when a job triggers a reconfiguration, the resource manager will check the cluster status and return the appropriate action: i) expand, if there are spare resources; ii) shrink, if queued jobs can be initiated; or iii) none, if no change can improve the global productivity. In this paper, we describe the internals of our framework and demonstrate how it reduces the global workload completion time along with providing a more efficient usage of the underlying resources. For this purpose, we present a thorough study of the adaptive workloads processing by showing the detailed behavior of our framework in representative experiments.},
	urldate = {2024-01-23},
	journal = {Parallel Computing},
	author = {Iserte, Sergio and Mayo, Rafael and Quintana-Ortí, Enrique S. and Beltran, Vicenç and Peña, Antonio J.},
	month = oct,
	year = {2018},
	keywords = {Adaptive workload, Dynamic reallocation, Job reconfiguration, MPI malleability, Smart resource utilization},
	pages = {54--66},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\PMHPNHHW\\Iserte et al. - 2018 - DMR API Improving cluster productivity by turning.pdf:application/pdf},
}

@article{iserte_dynamic_2019,
	title = {Dynamic reconfiguration of noniterative scientific applications: {A} case study with {HPG} aligner},
	volume = {33},
	copyright = {All rights reserved},
	issn = {1094-3420},
	shorttitle = {Dynamic reconfiguration of noniterative scientific applications},
	url = {https://doi.org/10.1177/1094342018802347},
	doi = {10.1177/1094342018802347},
	abstract = {Several studies have proved the benefits of job malleability, that is, the capacity of an application to adapt its parallelism to a dynamically changing number of allocated processors. The most remarkable advantages of executing malleable jobs as part of a high performance computer workload are the throughput increase and the more efficient utilization of the underlying resources. Malleability has been mostly applied to iterative applications where all the processes execute the same operations over different sets of data and with a balanced per process load. Unfortunately, not all scientific applications adhere to this process-level malleable job structure. There are scientific applications which are either noniterative or present an irregular per process load distribution. Unlike many other reconfiguration tools, the Dynamic Management of Resources Application Programming Interface (DMR API) provides the necessary flexibility to make malleable these out-of-target applications. In this article, we study the particular case of using the DMR API to generate a malleable version of HPG aligner, a distributed-memory noniterative genomic sequencer featuring an irregular communication pattern among processes. Through this first conversion of an out-of-target application to a malleable job, we both illustrate how the DMR API may be used to convert this type of applications into malleable and test the benefits of this conversion in production clusters. Our experimental results reveal an important reduction of the malleable HPG aligner jobs completion time compared to the original HPG aligner version. Furthermore, HPG aligner malleable workloads achieve a greater throughput than their fixed counterparts.},
	language = {en},
	number = {5},
	urldate = {2024-01-23},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Iserte, Sergio and Martínez, Héctor and Barrachina, Sergio and Castillo, Maribel and Mayo, Rafael and Peña, Antonio J},
	month = sep,
	year = {2019},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {804--816},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\T65XTICB\\Iserte et al. - 2019 - Dynamic reconfiguration of noniterative scientific.pdf:application/pdf},
}

@article{iserte_improving_2021,
	title = {Improving the management efficiency of {GPU} workloads in data centers through {GPU} virtualization},
	volume = {33},
	copyright = {© 2019 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5275},
	doi = {10.1002/cpe.5275},
	abstract = {Graphics processing units (GPUs) are currently used in data centers to reduce the execution time of compute-intensive applications. However, the use of GPUs presents several side effects, such as increased acquisition costs and larger space requirements. Furthermore, GPUs require a nonnegligible amount of energy even while idle. Additionally, GPU utilization is usually low for most applications. In a similar way to the use of virtual machines, using virtual GPUs may address the concerns associated with the use of these devices. In this regard, the remote GPU virtualization mechanism could be leveraged to share the GPUs present in the computing facility among the nodes of the cluster. This would increase overall GPU utilization, thus reducing the negative impact of the increased costs mentioned before. Reducing the amount of GPUs installed in the cluster could also be possible. However, in the same way as job schedulers map GPU resources to applications, virtual GPUs should also be scheduled before job execution. Nevertheless, current job schedulers are not able to deal with virtual GPUs. In this paper, we analyze the performance attained by a cluster using the remote Compute Unified Device Architecture middleware and a modified version of the Slurm scheduler, which is now able to assign remote GPUs to jobs. Results show that cluster throughput, measured as jobs completed per time unit, is doubled at the same time that the total energy consumption is reduced up to 40\%. GPU utilization is also increased.},
	language = {en},
	number = {2},
	urldate = {2024-01-23},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Iserte, Sergio and Prades, Javier and Reaño, Carlos and Silla, Federico},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.5275},
	keywords = {GPU, CUDA, InfiniBand, Slurm, data centers, rCUDA},
	pages = {e5275},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\DN6JYVRY\\Iserte et al. - 2021 - Improving the management efficiency of GPU workloa.pdf:application/pdf},
}

@article{iserte_dmrlib_2021,
	title = {{DMRlib}: {Easy}-{Coding} and {Efficient} {Resource} {Management} for {Job} {Malleability}},
	volume = {70},
	copyright = {All rights reserved},
	issn = {1557-9956},
	shorttitle = {{DMRlib}},
	url = {https://ieeexplore.ieee.org/document/9190024},
	doi = {10.1109/TC.2020.3022933},
	abstract = {Process malleability has proved to have a highly positive impact on the resource utilization and global productivity in data centers compared with the conventional static resource allocation policy. However, the non-negligible additional development effort this solution imposes has constrained its adoption by the scientific programming community. In this work, we present DMRlib, a library designed to offer the global advantages of process malleability while providing a minimalist MPI-like syntax. The library includes a series of predefined communication patterns that greatly ease the development of malleable applications. In addition, we deploy several scenarios to demonstrate the positive impact of process malleability featuring different scalability patterns. Concretely, we study two job submission modes (rigid and moldable) in order to identify the best-case scenarios for malleability using metrics such as resource allocation rate, completed jobs per second, and energy consumption. The experiments prove that our elastic approach may improve global throughput by a factor higher than 3x compared to the traditional workloads of non-malleable jobs.},
	number = {9},
	urldate = {2024-01-23},
	journal = {IEEE Transactions on Computers},
	author = {Iserte, Sergio and Mayo, Rafael and Quintana-Ortí, Enrique S. and Peña, Antonio J.},
	month = sep,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Computers},
	pages = {1443--1457},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\7H5IJ6XY\\Iserte et al. - 2021 - DMRlib Easy-Coding and Efficient Resource Managem.pdf:application/pdf},
}

@article{iserte_study_2020,
	title = {A study of the effect of process malleability in the energy efficiency on {GPU}-based clusters},
	volume = {76},
	copyright = {All rights reserved},
	issn = {1573-0484},
	url = {https://doi.org/10.1007/s11227-019-03034-x},
	doi = {10.1007/s11227-019-03034-x},
	abstract = {The adoption of graphic processor units (GPU) in high-performance computing (HPC) infrastructures determines, in many cases, the energy consumption of those facilities. For this reason, an efficient management and administration of the GPU-enabled clusters is crucial for the optimum operation of the cluster. The main aim of this work is to study and design efficient mechanisms of job scheduling across GPU-enabled clusters by leveraging process malleability techniques, able to reconfigure running jobs, depending on the cluster status. This paper presents a model that improves the energy efficiency when processing a batch of jobs in an HPC cluster. The model is validated through the MPDATA algorithm, as a representative example of stencil computation used in numerical weather prediction. The proposed solution applies the efficiency metrics obtained in a new reconfiguration policy aimed at job arrays. This solution allows the reduction in the processing time of workloads up to 4.8 times and reduction in the energy consumption up to 2.4 times the cluster compared to the traditional job management, where jobs are not reconfigured during their execution.},
	language = {en},
	number = {1},
	urldate = {2024-01-23},
	journal = {The Journal of Supercomputing},
	author = {Iserte, Sergio and Rojek, Krzysztof},
	month = jan,
	year = {2020},
	keywords = {Job reconfiguration, MPI malleability, Dynamic resource management, Heterogeneous programming model, Job array-aware scheduling, MPDATA algorithm},
	pages = {255--274},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\N7MGD897\\Iserte and Rojek - 2020 - An study of the effect of process malleability in .pdf:application/pdf},
}

@article{catalan_leveraging_2021,
	title = {Leveraging teaching on demand: {Approaching} {HPC} to undergrads},
	volume = {156},
	copyright = {All rights reserved},
	issn = {0743-7315},
	shorttitle = {Leveraging teaching on demand},
	url = {https://www.sciencedirect.com/science/article/pii/S0743731521001271},
	doi = {10.1016/j.jpdc.2021.05.015},
	abstract = {High Performance Computing (HPC) is a highly demanded discipline in companies and institutions. However, as students and also afterwards as professors, we observed a lack of HPC related content in the engineering degrees at our university, including Computer Science. Thus, we designed and offered the engineering students a non-mandatory course entitled “Build your own cluster employing Raspberry Pi” to provide the students with HPC skills. With this course, we covered the basics of supercomputing (hardware, networking, software tools, performance evaluation, cluster management, etc.). This was possible thanks to leveraging the flexibility and versatility of Raspberry Pi devices, and the students' motivation that arose from the hands-on experience. Moreover, the course included a “Teaching on demand” component to let the attendees choose a field to explore, based on their own interests. In this paper, we offer all the details to let anyone fully reproduce the course. Besides, we analyze and evaluate the methodology that let us fulfill our objectives: increase the students' HPC skills and knowledge in such a way that they feel capable of utilizing it in their mid-term professional career.},
	urldate = {2024-01-23},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Catalán, Sandra and Carratalá-Sáez, Rocío and Iserte, Sergio},
	month = oct,
	year = {2021},
	keywords = {Computational cluster, Parallel and distributed computing, Raspberry Pi, System administration, Undergrad teaching},
	pages = {148--162},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\GV9T9LF9\\Catalán et al. - 2021 - Leveraging teaching on demand Approaching HPC to .pdf:application/pdf},
}

@article{iserte_modeling_2021,
	title = {Modeling of wastewater treatment processes with hydrosludge},
	volume = {93},
	copyright = {© 2021 Water Environment Federation},
	issn = {1554-7531},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wer.1656},
	doi = {10.1002/wer.1656},
	abstract = {The pressure for Water Resource Recovery Facilities (WRRF) operators to efficiently treat wastewater is greater than ever because of the water crisis, produced by the climate change effects and more restrictive regulations. Technicians and researchers need to evaluate WRRF performance to ensure maximum efficiency. For this purpose, numerical techniques, such as CFD, have been widely applied to the wastewater sector to model biological reactors and secondary settling tanks with high spatial and temporal accuracy. However, limitations such as complexity and learning curve prevent extending CFD usage among wastewater modeling experts. This paper presents HydroSludge, a framework that provides a series of tools that simplify the implementation of the processes and workflows in a WRRF. This work leverages HydroSludge to preprocess existing data, aid the meshing process, and perform CFD simulations. Its intuitive interface proves itself as an effective tool to increase the efficiency of wastewater treatment. Practitioner points This paper introduces a software platform specifically oriented to WRRF, named HydroSludge, which provides easy access to the most widespread and leading CFD simulation software, OpenFOAM. Hydrosludge is intended to be used by WRRF operators, bringing a more wizard-like, automatic, and intuitive usage. Meshing assistance, submersible mixers, biological models, and distributed parallel computing are the most remarkable features included in HydroSludge. With the provided study cases, HydroSludge has proven to be a crucial tool for operators, managers, and researchers in WRRF.},
	language = {en},
	number = {12},
	urldate = {2024-01-23},
	journal = {Water Environment Research},
	author = {Iserte, Sergio and Carratalà, Pablo and Arnau, Rosario and Martínez-Cuenca, Raúl and Barreda, Paloma and Basiero, Luís and Climent, Javier and Chiva, Sergio},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wer.1656},
	keywords = {ASM1, meshing assistance, OpenFOAM, parallel computing, submersible mixers},
	pages = {3049--3063},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\PQEZ7G4G\\Iserte et al. - 2021 - Modeling of wastewater treatment processes with hy.pdf:application/pdf},
}

@article{aliaga_survey_2022,
	title = {A {Survey} on {Malleability} {Solutions} for {High}-{Performance} {Distributed} {Computing}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/10/5231},
	doi = {10.3390/app12105231},
	abstract = {Maintaining a high rate of productivity, in terms of completed jobs per unit of time, in High-Performance Computing (HPC) facilities is a cornerstone in the next generation of exascale supercomputers. Process malleability is presented as a straightforward mechanism to address that issue. Nowadays, the vast majority of HPC facilities are intended for distributed-memory applications based on the Message Passing (MP) paradigm. For this reason, many efforts are based on the Message Passing Interface (MPI), the de facto standard programming model. Malleability aims to rescale executions on-the-fly, in other words, reconfigure the number and layout of processes in running applications. Process malleability involves resources reallocation within the HPC system, handling processes of the application, and redistributing data among those processes to resume the execution. This manuscript compiles how different frameworks address process malleability, their main features, their integration in resource management systems, and how they may be used in user codes. This paper is a detailed state-of-the-art devised as an entry point for researchers who are interested in process malleability.},
	language = {en},
	number = {10},
	urldate = {2024-01-23},
	journal = {Applied Sciences},
	author = {Aliaga, Jose I. and Castillo, Maribel and Iserte, Sergio and Martín-Álvarez, Iker and Mayo, Rafael},
	month = jan,
	year = {2022},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive workloads, data redistribution, exascale, job reconfiguration, MPI, resource management},
	pages = {5231},
	file = {Full Text PDF:C\:\\Users\\siser\\Zotero\\storage\\MJN53Q96\\Aliaga et al. - 2022 - A Survey on Malleability Solutions for High-Perfor.pdf:application/pdf},
}

@article{iserte_accelerating_2022,
	title = {Accelerating urban scale simulations leveraging local spatial {3D} structure},
	volume = {62},
	copyright = {All rights reserved},
	issn = {1877-7503},
	url = {https://www.sciencedirect.com/science/article/pii/S1877750322001326},
	doi = {10.1016/j.jocs.2022.101741},
	abstract = {This paper presents a hybrid methodology for accelerating Computational Fluid Dynamics (CFD) simulations intertwining inferences from deep neural networks (DNN). The strategy leverages the local spatial data of the velocity field to leverage three-dimensional convolutional kernels within DNN. The hybrid workflow is composed of two-step cycles where CFD solvers calculations are utilized to feed predictive models, whose inferences, in turn, accelerate the simulation of the fluid evolution compared with traditional CFD. This approach has proved to reduce 30\% time-to-solution in an urban scale study case, which leads to generating massive datasets at a fraction of the cost.},
	urldate = {2024-01-23},
	journal = {Journal of Computational Science},
	author = {Iserte, Sergio and Macías, Aina and Martínez-Cuenca, Raúl and Chiva, Sergio and Paredes, Roberto and Quintana-Ortí, Enrique S.},
	month = jul,
	year = {2022},
	keywords = {Computational fluid dynamics, Convolutional neural network, Data-driven simulations, High performance computing, Structured mesh},
	pages = {101741},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\7F5HYXAE\\Iserte et al. - 2022 - Accelerating urban scale simulations leveraging lo.pdf:application/pdf},
}

@article{iserte_study_2023,
	title = {A study on the performance of distributed training of data-driven {CFD} simulations},
	volume = {37},
	copyright = {All rights reserved},
	issn = {1094-3420},
	url = {https://doi.org/10.1177/10943420231160557},
	doi = {10.1177/10943420231160557},
	abstract = {Data-driven methods for computer simulations are blooming in many scientific areas. The traditional approach to simulating physical behaviors relies on solving partial differential equations (PDEs). Since calculating these iterative equations is highly both computationally demanding and time-consuming, data-driven methods leverage artificial intelligence (AI) techniques to alleviate that workload. Data-driven methods have to be trained in advance to provide their subsequent fast predictions; however, the cost of the training stage is non-negligible. This article presents a predictive model for inferencing future states of a specific fluid simulation that serves as a use case for evaluating different training alternatives. Particularly, this study compares the performance of only CPU, multi-GPU, and distributed approaches for training a time series forecasting deep learning model. With some slight code adaptations, results show and compare, in different implementations, the benefits of distributed GPU-enabled training for predicting high-accuracy states in a fraction of the time needed by the computational fluid dynamics solver.},
	language = {en},
	number = {5},
	urldate = {2024-01-23},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Iserte, Sergio and González-Barberá, Alejandro and Barreda, Paloma and Rojek, Krzysztof},
	month = sep,
	year = {2023},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {503--515},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\MUJYY8HL\\Iserte et al. - 2023 - A study on the performance of distributed training.pdf:application/pdf},
}

@article{martin-alvarez_dynamic_2023,
	title = {Dynamic spawning of {MPI} processes applied to malleability},
	copyright = {All rights reserved},
	issn = {1094-3420},
	url = {https://doi.org/10.1177/10943420231176527},
	doi = {10.1177/10943420231176527},
	abstract = {Malleability allows computing facilities to adapt their workloads through resource management systems to maximize the throughput of the facility and the efficiency of the executed jobs. This technique is based on reconfiguring a job to a different resource amount during execution and then continuing with it. One of the stages of malleability is the dynamic spawning of processes in execution time, where different decisions in this stage will affect how the next stage of data redistribution is performed, which is the most time-consuming stage. This paper describes different methods and strategies, defining eight different alternatives to spawn processes dynamically and indicates which one should be used depending on whether a strong or weak scaling application is being used. In addition, it is described for both types of applications which strategies benefit most the application performance or the system productivity. The results show that reducing the number of spawning processes by reusing the older ones can reduce reconfiguration time compared to the classical method by up to 2.6 times for expanding and up to 36 times for shrinking. Furthermore, the asynchronous strategy requires analysing the impact of oversubscription on application performance.},
	language = {en},
	urldate = {2024-01-23},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Martín-Álvarez, Iker and Aliaga, José I and Castillo, Maribel and Iserte, Sergio and Mayo, Rafael},
	month = may,
	year = {2023},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {10943420231176527},
	file = {Full Text:C\:\\Users\\siser\\Zotero\\storage\\IGQ4YQI8\\Martín-Álvarez et al. - 2023 - Dynamic spawning of MPI processes applied to malle.pdf:application/pdf},
}

@article{rosciszewski_optimizing_2023,
	title = {Optimizing throughput of {Seq2Seq} model training on the {IPU} platform for {AI}-accelerated {CFD} simulations},
	volume = {147},
	copyright = {All rights reserved},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001784},
	doi = {10.1016/j.future.2023.05.004},
	abstract = {Intelligence Processing Units (IPU) have proven useful for many AI applications. In this paper, we evaluate them within the emerging field of AI for simulation, where traditional numerical simulations are supported by artificial intelligence approaches. We focus specifically on a program for training machine learning models supporting a computational fluid dynamics application. We use custom TensorFlow provided by the Poplar Software Development Kit to adapt the program for the IPU-POD16 platform and investigate its ease of use and performance scalability. Training a model on data from OpenFOAM simulations allows us to get accurate simulation state predictions in test time. We describe how to optimize multi-threading runtime options and utilize the popdist library to overcome a performance bottleneck in feeding training data to the IPU on the host side. Due to communication overheads, using data parallelism to utilize two IPUs instead of one does not improve the throughput. However, once the intra-IPU costs have been paid, the hardware capabilities for inter-IPU communication allow for good scalability. Increasing the number of IPUs from two to 16 improves the throughput from 560.8 to 2805.8 samples/s. Additionally, the experimental results show that reducing the precision of input data storage from FP32 to FP16 allows to improve training throughput by 12\%, while tuning selected runtime variables, by up to 6.3\%.},
	urldate = {2024-01-23},
	journal = {Future Generation Computer Systems},
	author = {Rościszewski, Paweł and Krzywaniak, Adam and Iserte, Sergio and Rojek, Krzysztof and Gepner, Paweł},
	month = oct,
	year = {2023},
	keywords = {Computational fluid dynamics, Deep learning, Distributed computing, High-performance computing, Intelligence Processing Unit, Time-series prediction},
	pages = {149--162},
}

@article{martinez-cuenca_use_2023,
	title = {On the use of deep learning and computational fluid dynamics for the estimation of uniform momentum source components of propellers},
	volume = {26},
	copyright = {All rights reserved},
	issn = {2589-0042},
	url = {https://www.sciencedirect.com/science/article/pii/S258900422302374X},
	doi = {10.1016/j.isci.2023.108297},
	abstract = {This article proposes a novel method based on Deep Learning for the resolution of uniform momentum source terms in the Reynolds-Averaged Navier-Stokes equations. These source terms can represent several industrial devices (propellers, wind turbines, and so forth) in Computational Fluid Dynamics simulations. Current simulation methods require huge computational power, rely on strong assumptions or need additional information about the device that is being simulated. In this first approach to the new method, a Deep Learning system is trained with hundreds of Computational Fluid Dynamics simulations with uniform momemtum sources so that it can compute the one representing a given propeller from a reduced set of flow velocity measurements near it. Results show an overall relative error below the 5\% for momentum sources for uniform sources and a moderate error when describing real propellers. This work will allow to simulate more accurately industrial devices with less computational cost.},
	number = {12},
	urldate = {2024-01-23},
	journal = {iScience},
	author = {Martínez-Cuenca, Raúl and Luis-Gómez, Jaume and Iserte, Sergio and Chiva, Sergio},
	month = dec,
	year = {2023},
	keywords = {Artificial intelligence, Industrial engineering},
	pages = {108297},
}

@article{iserte_gsaas_2018,
	title = {{GSaaS}: {A} {Service} to {Cloudify} and {Schedule} {GPUs}},
	volume = {6},
	copyright = {All rights reserved},
	issn = {2169-3536},
	shorttitle = {{GSaaS}},
	url = {https://ieeexplore.ieee.org/document/8410512},
	doi = {10.1109/ACCESS.2018.2855261},
	abstract = {Cloud technology is an attractive infrastructure solution that provides customers with an almost unlimited on-demand computational capacity using a pay-per-use approach, and allows data centers to increase their energy and economic savings by adopting a virtualized resource sharing model. However, resources such as graphics processing units (GPUs), have not been fully adapted to this model. Although, general-purpose computing on graphics processing units (GPGPU) is becoming more and more popular, cloud providers lack of flexibility to manage accelerators, because of the extended use of peripheral component interconnect (PCI) passthrough techniques to attach GPUs to virtual machines (VMs). For this reason, we design, develop, and evaluate a service that provides a complete management of cloudified GPUs (cGPUs) in public cloud platforms. Our solution enables an effective, anonymous, and transparent access from VMs to cGPUs that are previously scheduled and assigned by a full resource manager, taking into account new GPU selection policies and new working modes based on the locality of the physical accelerators and the exclusivity when accessing them. This easy-to-adopt tool improves the resource availability through different cGPUs configurations for end-users, whilst cloud providers are able to achieve a better utilization of their infrastructures and offer more competitive services. Scalability results in a real cloud environment demonstrate that our solution introduces a virtually null overhead in the deployment of VMs. Besides, performance experiments reveal that GPU-enabled clusters based on cloud infrastructures can benefit from our proposal not only exploiting better the accelerators, but also serving more jobs requests per unit of time.},
	urldate = {2024-01-23},
	journal = {IEEE Access},
	author = {Iserte, Sergio and Peña-Ortiz, Raúl and Gutiérrez-Aguado, Juan and Claver, Jose M. and Mayo, Rafael},
	year = {2018},
	note = {Conference Name: IEEE Access},
	pages = {39762--39774},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\siser\\Zotero\\storage\\5XTCCSVR\\8410512.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\siser\\Zotero\\storage\\5L3XKLRW\\Iserte et al. - 2018 - GSaaS A Service to Cloudify and Schedule GPUs.pdf:application/pdf},
}

@phdthesis{iserte_high-throughput_2018,
	address = {Castelló de la Plana},
	type = {{PhD} {Thesis}},
	title = {High-throughput {Computation} through {Efficient} {Resource} {Management}},
	copyright = {All rights reserved},
	url = {http://hdl.handle.net/10803/664128},
	school = {Universitat Jaume I},
	author = {Iserte, Sergio},
	month = nov,
	year = {2018},
	doi = {10.6035/14101.2018.176272},
}

@inproceedings{usman_dpu_2023,
	address = {New York, NY, USA},
	series = {{SC}-{W} '23},
	title = {{DPU} {Offloading} {Programming} with the {OpenMP} {API}},
	copyright = {All rights reserved},
	isbn = {9798400707858},
	url = {https://doi.org/10.1145/3624062.3624165},
	doi = {10.1145/3624062.3624165},
	abstract = {Data processing units (DPUs) as network co-processors are an emerging trend in our community, with plenty of opportunities yet to be explored. These have been generally used as domain-specific accelerators transparent to application developers; In the HPC field, DPUs have been used as MPI accelerators, but also to offload some tasks from the general-purpose processor. However, the latter required application developers to deploy MPI ranks in the DPUs, as if they were remote (weak) compute nodes, hence considerably hindering programmability. The wide adoption of OpenMP as the threading model in the HPC arena, along with that of GPU accelerators, is making OpenMP offloading to GPUs a wide trend for HPC applications. In this paper we introduce, for the first time in the literature, OpenMP offloading support for network co-processor DPUs. We present our design in LLVM to support OpenMP standard offloading semantics and discuss the programming productivity advantages with respect to the existing MPI-based programming model. We also provide the corresponding performance analysis demonstrating competitive results in comparison with the MPI baseline.},
	urldate = {2024-01-24},
	booktitle = {Proceedings of the {SC} '23 {Workshops} of {The} {International} {Conference} on {High} {Performance} {Computing}, {Network}, {Storage}, and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Usman, Muhammad and Iserte, Sergio and Ferrer, Roger and Peña, Antonio José},
	month = nov,
	year = {2023},
	keywords = {BlueField, DPUs, ODOS, OpenMP Offloading},
	pages = {884--891},
}
